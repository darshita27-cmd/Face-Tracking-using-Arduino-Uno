{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMtJxj6CLXVlwzP0FUJESa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/darshita27-cmd/Face-Tracking-using-Arduino-Uno/blob/main/face_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 # for camera feed, objection detection\n",
        "import numpy as np # for mathematical and array\n",
        "import time # for stopping camera feed or opening camera for a particular duration\n",
        "classifierFace=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\") # using predefined cascade from opencv for face detection\n",
        "videoCam=cv2.VideoCapture(0) # getting the video feed. 0 is used to get the video feed of the default camera. 1 or 2 can also be used to secify which camera needed\n",
        "\n",
        "if not videoCam.isOpened():\n",
        "  print(\"the camera is not working\")\n",
        "  exit() # exitq terminates the whole program whereas break terminates only the loop\n",
        "buttonispressed=False\n",
        "while(buttonispressed==False):\n",
        "  ret,framework=videoCam.read() # ret returns a boolean value. ret returns True if campera can be accesed and false if can't access the camera. framework captures the image in array. array will be 2D if captured image is grayscale image ( width and height). #3D if its a coloures image (height,width,color chanels)\n",
        "\n",
        "  if ret==True: # it means if camera is accesed\n",
        "    gray=cv2.cvtColor(framework,cv2.COLOR_BGR2GRAY) #cv2.cvtcolor converts colored image to gray. cv2.COLOR_BGR2GRAY is a flag. by default it is for colored image.\n",
        "    dafFace=classifierFace.detectMultiScale(gray,scaleFactor=1.3,minNeighbors=2) # scaleFactor=1.3 means that 30% of image size will be reduced at each scale. minNeighbors=2 desides how many neighbors each candidate rectangle should have. higher value results in fewer detections with hight quality. 2 means that a rectangle must be detected at least twice to be considered valid detection.\n",
        "    for (x,y,w,h) in dafFace: # x = x axis, y=y axis, w= width, h=height these will make the boxes around each face detected\n",
        "      cv2.rectangle(framework,(x,y),(x+w,y+h),(0,255,0),2) # freamework is the image on which to draw rectangle. (x,y) is the top left corner of the rectangle. (x+w,y+h) is the bottom right in the rectangle. (0,255,0) 0 is for blue, 255 is for green, 0 is for red. so the rectngle will be green in color. 2 is the border thickness of the rectangle\n",
        "\n",
        "    teks = 'number of faces detected = ' + str(len(dafFace)) # diq;dn't calculate the number of rectangles becuse harcase we are usig already returns an array with detectMultiSacle.\n",
        "    font=cv2.FONT_HERSHEY_SIMPLEX\n",
        "    cv2.putText(framework, teks, (0,30),font,1,(255,0,0),1)\n",
        "    cv2.imshow('results',framework)\n",
        "    if cv2.waitKey(1) & 0xFF==ord('q'):# cv2.waitKey returns the ASCII value of the key pressed.  &0xFF is so that it can be used in different windows. ord('q') returns the ASCII of q. if the q key is pressed than it returns True\n",
        "      buttonispressed = True\n",
        "      break\n",
        "\n",
        "videoCam.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ],
      "metadata": {
        "id": "paBRtgOVn-Q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}